{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # Import libraries to be used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import plotly.express as px\n",
    "import datetime\n",
    "import emoji\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of a chat line "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 11/10/19, 12:01 - Nishant Jio: Monday ko sunil ko dekhe attendance maang lena\n",
    "## {Date}, {Time} - {Author}: {Message}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Detecting {Date} and {Time} tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In order to detect if a line of text is a new message or belongs to a multi-line message, we will have to check if that line begins with a Date and Time, for which we will need a little bit of regular expression (regex)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def startsWithDate(s):\n",
    "    pattern = '^([0-2][0-9]|(3)[0-1])(\\/)(((0)[0-9])|((1)[0-2]))(\\/)(\\d{2}|\\d{4}), ([0-9][0-9]):([0-9][0-9]) -'\n",
    "    result = re.match(pattern, s)\n",
    "    if result:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting the {Author} token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Once again, it will require some more regular expression matching. Objective is to detect the author of this message. While there could be a variety of patterns depending on how you have saved your friendsâ€™ names in any phone contacts app, the most commonly used patterns I have identified are as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def startsWithAuthor(s):\n",
    "    patterns = [\n",
    "        '([\\w]+):',                        # First Name\n",
    "        '([\\w]+[\\s]+[\\w]+):',              # First Name + Last Name\n",
    "        '([\\w]+[\\s]+[\\w]+[\\s]+[\\w]+):',    # First Name + Middle Name + Last Name\n",
    "        '([+]\\d{2} \\d{5} \\d{5}):',         # Mobile Number (India)\n",
    "        '([+]\\d{2} \\d{3} \\d{3} \\d{4}):',   # Mobile Number (US)\n",
    "        '([+]\\d{2} \\d{4} \\d{7})'           # Mobile Number (Europe)\n",
    "        '([\\w]+)[\\u263a-\\U0001f999]+:',    # Name and Emoji  \n",
    "    ]\n",
    "    pattern = '^' + '|'.join(patterns)\n",
    "    result = re.match(pattern, s)\n",
    "    if result:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting and Combining tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that I am able to identify the Date, Time, Author and Message tokens in a single message, it is time to split each line based on the separator tokens like commas (,), hyphens(-), colons(:) and spaces( ), so that the required tokens can be extracted and saved in a dataframe. This time, let me invert things by highlighting the separator tokens instead of the Date, Time, Author and Message tokens:\n",
    "#### Date{Comma }Time{ Hyphen }Author{Colon }Message\n",
    "#### 11/10/19{, }12:01{ - }Nishant Jio{: }Monday ko sunil ko dekhe attendance maang lena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataPoint(line):\n",
    "    # line = 11/10/19, 12:01 - Nishant Jio: Monday ko sunil ko dekhe attendance maang lena\n",
    "    \n",
    "    splitLine = line.split(' - ') # splitLine = ['11/10/19, 12:01', 'Nishant Jio: Monday ko sunil ko dekhe attendance maang lena']\n",
    "    \n",
    "    dateTime = splitLine[0] # dateTime = '11/10/19, 12:01'\n",
    "    \n",
    "    date, time = dateTime.split(', ') # date = '11/10/19'; time = '12:01'\n",
    "    \n",
    "    message = ' '.join(splitLine[1:]) # message = 'Nishant Jio: Monday ko sunil ko dekhe attendance maang lena'\n",
    "    \n",
    "    if startsWithAuthor(message): # True\n",
    "        splitMessage = message.split(': ') # splitMessage = ['Nishant Jio', 'Monday ko sunil ko dekhe attendance maang lena']\n",
    "        author = splitMessage[0] # author = 'Nishant Jio'\n",
    "        message = ' '.join(splitMessage[1:]) # message = 'Monday ko sunil ko dekhe attendance maang lena'\n",
    "    else:\n",
    "        author = None\n",
    "    return date, time, author, message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing the entire file and handling Multi-Line Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsedData = [] # List to keep track of data so it can be used by a Pandas dataframe\n",
    "\n",
    "conversationPath = 'group.txt'   # The path of txt file\n",
    "\n",
    "with open(conversationPath, encoding=\"utf-8\") as fp:\n",
    "    fp.readline() # Skipping first line of the file (usually contains information about end-to-end encryption)\n",
    "        \n",
    "    messageBuffer = [] # Buffer to capture intermediate output for multi-line messages\n",
    "    date, time, author = None, None, None # Intermediate variables to keep track of the current message being processed\n",
    "    \n",
    "    while True:\n",
    "        line = fp.readline() \n",
    "        \n",
    "        if not line:    # Stop reading further if end of file has been reached\n",
    "            break\n",
    "        line = line.strip()    # Guarding against erroneous leading and trailing whitespaces\n",
    "        \n",
    "        if startsWithDate(line):      # If a line starts with a Date Time pattern, then this indicates the beginning of a new message\n",
    "            if len(messageBuffer) > 0:     # Check if the message buffer contains characters from previous iterations\n",
    "                parsedData.append([date, time, author, ' '.join(messageBuffer)])    # Save the tokens from the previous message in parsedData\n",
    "                \n",
    "            messageBuffer.clear()    # Clear the message buffer so that it can be used for the next message\n",
    "            date, time, author, message = getDataPoint(line)     # Identify and extract tokens from the line\n",
    "            messageBuffer.append(message)    # Append message to buffer\n",
    "            \n",
    "        else:\n",
    "            messageBuffer.append(line)    # If a line doesn't start with a Date Time pattern, then it is part of a multi-line message. So, just append to buffer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(parsedData, columns=['Date', 'Time', 'Author', 'Message'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Describe data\n",
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "media_messages = df[df['Message'] == '<Media omitted>'].shape[0]\n",
    "media_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group Wise Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One thing to notice here is, we are exporting the chat without media. So in place of media messages, we will find the phrase Media omitted>. With the help of this, we can find the total number of media messages shared in the group. For finding the total emojis used we will be using the emoji library. We will also create a separate column emojis that consists of only the emojis for that particular message. For finding the total number of links shared we will write a regex pattern and use the re library in python to identify URLs in a given message. We will also create a separate column urlcount that consists of the count of URLs in a particular message. All the additional created columns will be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_count(text):\n",
    "\n",
    "    emoji_list = []\n",
    "    data = re.findall(r'\\S', text)\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI for char in word):\n",
    "            emoji_list.append(word)\n",
    "\n",
    "    return emoji_list\n",
    "\n",
    "total_messages = df.shape[0]\n",
    "media_messages = df[df['Message'] == '<Media omitted>'].shape[0]\n",
    "\n",
    "df['emoji'] = df.Message.apply(split_count)\n",
    "emojis = sum(df['emoji'].str.len())\n",
    "\n",
    "URLPATTERN = r'(https?://\\S+)'\n",
    "df['urlcount'] = df.Message.apply(lambda x: re.findall(URLPATTERN, x)).str.len()\n",
    "links = np.sum(df.urlcount)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "media_messages_df = df[df['Message'] == '<Media omitted>']\n",
    "messages_df = df.drop(media_messages_df.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_df['Letter_Count'] = messages_df['Message'].apply(lambda s : len(s))\n",
    "messages_df['Word_Count'] = messages_df['Message'].apply(lambda s : len(s.split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Highly talkative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_value_counts = messages_df['Author'].value_counts()         # Number of messages per author\n",
    "top_author_value_counts = author_value_counts.head(3)     # Number of messages per author for the top 10 most active authors\n",
    "top_author_value_counts.plot.barh()                       # Plot a bar chart using pandas built-in plotting apis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mysterious Messages with No Authors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_df = messages_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_authors_df = messages_df[messages_df['Author'].isnull()]\n",
    "null_authors_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emojis stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Total Emojis\n",
    "\n",
    "total_emojis_list = list(set([a for b in messages_df.emoji for a in b]))\n",
    "total_emojis = len(total_emojis_list)\n",
    "print(total_emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Most used Emoji in Group\n",
    "\n",
    "total_emojis_list = list([a for b in messages_df.emoji for a in b])\n",
    "emoji_dict = dict(Counter(total_emojis_list))\n",
    "emoji_dict = sorted(emoji_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "emoji_df = pd.DataFrame(emoji_dict, columns=['emoji', 'count'])\n",
    "emoji_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Emoji distribution\n",
    "\n",
    "fig = px.pie(emoji_df, values='count', names='emoji',\n",
    "             title='Emoji Distribution')\n",
    "fig.update_traces(textposition='inside', textinfo='percent+label')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a list of unique Authors \n",
    "l = messages_df.Author.unique()\n",
    "for i in range(len(l)):\n",
    "  dummy_df = messages_df[messages_df['Author'] == l[i]]\n",
    "  total_emojis_list = list([a for b in dummy_df.emoji for a in b])\n",
    "  emoji_dict = dict(Counter(total_emojis_list))\n",
    "  emoji_dict = sorted(emoji_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "  print('Emoji Distribution for', l[i])\n",
    "  author_emoji_df = pd.DataFrame(emoji_dict, columns=['emoji', 'count'])\n",
    "  fig = px.pie(author_emoji_df, values='count', names='emoji')\n",
    "  fig.update_traces(textposition='inside', textinfo='percent+label')\n",
    "  fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" \".join(review for review in messages_df.Message)\n",
    "print (\"There are {} words in all the messages.\".format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(STOPWORDS)\n",
    "stopwords.update([\"ra\", \"ga\", \"na\", \"ani\", \"em\", \"ki\", \"ah\",\"ha\",\"la\",\"eh\",\"ne\",\"le\"])\n",
    "  # Generate a word cloud image\n",
    "wordcloud = WordCloud(stopwords=stopwords, background_color=\"white\").generate(text)\n",
    "  # Display the generated image:\n",
    "  # the matplotlib way:\n",
    "  \n",
    "plt.figure( figsize=(10,5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of messages as time moves on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_df = messages_df.groupby(\"Date\").sum()\n",
    "date_df.reset_index(inplace=True)\n",
    "fig = px.line(date_df, x=\"Date\", y=\"Word_Count\", title='Number of Messages as time moves on.')\n",
    "fig.update_xaxes(nticks=20)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dayofweek(i):\n",
    "  l = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
    "  return l[i];\n",
    "day_df=pd.DataFrame(df[\"Message\"])\n",
    "day_df['day_of_date'] = df['Date'].dt.weekday\n",
    "day_df['day_of_date'] = day_df[\"day_of_date\"].apply(dayofweek)\n",
    "day_df[\"messagecount\"] = 1\n",
    "day = day_df.groupby(\"day_of_date\").sum()\n",
    "day.reset_index(inplace=True)\n",
    "\n",
    "fig = px.line_polar(day, r='messagecount', theta='day_of_date', line_close=True)\n",
    "fig.update_traces(fill='toself')\n",
    "fig.update_layout(\n",
    "  polar=dict(\n",
    "    radialaxis=dict(\n",
    "      visible=True,\n",
    "      range=[0,6000]\n",
    "    )),\n",
    "  showlegend=False\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most happening days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_df['Date'].value_counts().head(10).plot.barh()\n",
    "plt.xlabel('Number of Messages')\n",
    "plt.ylabel('Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When are the group members most active?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_df['Time'].value_counts().head(10).plot.barh() \n",
    "plt.xlabel('Number of messages')\n",
    "plt.ylabel('Time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## What is the most suitable hour of the day at which to message to increase your chances of getting a response from someone?\n",
    "\n",
    "messages_df['Hour'].value_counts().head(10).sort_index(ascending=False).plot.barh() # Top 10 Hours of the day during which the most number of messages were sent\n",
    "plt.xlabel('Number of messages')\n",
    "plt.ylabel('Hour of Day')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the most common number of letters in a message?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 2))\n",
    "letter_count_value_counts = messages_df['Letter_Count'].value_counts()\n",
    "top_40_letter_count_value_counts = letter_count_value_counts.head(40)\n",
    "top_40_letter_count_value_counts.plot.bar()\n",
    "plt.xlabel('Letter count')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### The long analysis comes to an end! In this code , I tried to analyze our Whatsapp group chats using Python and Plotly. I have analyze the pssible questions one can ask for whats app chat analisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
